# KV-cache

# KV-cacheとパラメータの関係

| 要素 | 役割（モデルの処理） | 役割（料理の例え） |
| :--- | :--- | :--- |
| **パラメータ（知識）** | 一般的な知識、言語の法則。どの単語が次の単語として適切かを判断する基準。 | 料理人の内在する知識。「塩と砂糖を間違えない」「魚は焼く」といった一般的な料理の法則。 |
| **$\text{KV}$ キャッシュ（文脈）** | 現在の会話や入力プロンプトという一時的な文脈。次に生成する単語が現在の文脈に合致しているかを確認する索引。 | 現在のレシピや直前の動作という一時的な作業文脈。「今回は塩ではなく砂糖を使う」といった、この料理固有の文脈。 |

エンコーダー・デコーダー型LLMが推論を行う際、モデルの知識と文脈は明確に分離された構造を持ちます。モデルが持つ一般的な知識、言語の法則、世界の事実などは、学習済みパラメータ（重み）に永続的に保持されており、これが知識の源泉となります。<br>
一方、$\mathbf{KV}$は一時的な文脈情報として、推論中に生成されます。この $\text{KV}$ の生成には二つのステップがあります。まず、入力文が与えられると、エンコーダーがこのパラメータを用いて、入力文の深い意味を符号化したエンコーダー $\mathbf{KV}$ を一度だけ生成します。このエンコーダー $\mathbf{KV}$ は、元の入力文が変わらない限り固定されており、再計算の必要はありませんが、デコーダーが生成の全ステップで参照できるようにメモリ上にキャッシュされます。<br>
次に、応答の生成が始まると、デコーダーはパラメータを用いつつ、自身の自己注意機構を通じて出力履歴の $\mathbf{KV}$ を逐次生成します。このデコーダー $\mathbf{KV}$ は、新しいトークンが生成されるたびに内容が変化（成長）するため、計算の重複を避ける目的でKVキャッシュに保存・更新されます。もしこの $\text{KV}$ キャッシュがなかった場合、モデルは新しいトークンを予測するたびに、既に出力した文章全体（過去の全てのトークン）をデコーダーに再入力し、全ての $\mathbf{K}$ と $\mathbf{V}$ を最初から最後まで、すべて再計算しなければならず、計算コストが二乗で増大し、実用的な応答速度が得られなくなります。最終的に、デコーダーはこの二つの $\text{KV}$（不変のエンコーダー $\text{KV}$ と可変のデコーダー $\text{KV}$）を文脈情報として参照しつつ、自身のパラメータに保持された知識を活用することで、正確な応答を生成します。

# NVIDIA DynamoによるKVキャッシュボトルネックの解決

NVIDIA Dynamoは、大規模言語モデル（LLM）の推論における最大の制約であった**KVキャッシュのメモリ制約**を解決するための画期的な技術です。この技術は、「KVキャッシュをストレージに保存する」という課題を、推論フレームワークのレベルで実現します。

## 1. Dynamoが実現するストレージ拡張

Dynamoの核となる機能は、従来のGPUメモリ（VRAM）に限定されていた $\text{KV}$ キャッシュの保存先を、以下の複数のストレージ階層に**オフロード（拡張）**可能にした点です。

1.  **CPU RAM**（ホストメモリ）
2.  **SSD**（ソリッドステートドライブ）
3.  **ネットワークストレージ**

この拡張により、LLMの「記憶容量」が飛躍的に増大し、**長文コンテキスト**や**高並行性ワークロード**（多数のユーザーとの同時会話）への対応力が大幅に向上します。

## 2. 解決される主要な課題

| 従来の課題（GPUメモリ制約下） | Dynamoによる解決 |
| :--- | :--- |
| **コンテキスト長** | VRAM容量に依存するため、長文処理に限界があった。 | **外部ストレージに拡張**し、LLMの「記憶容量」を飛躍的に向上。 |
| **セッション再利用** | 会話の再開時に、履歴テキストを再入力し $\text{KV}$ を**再計算**する必要があった。 | **永続的な $\text{KV}$ キャッシュの再利用**により、初回トークン生成時間（TTFT）が大幅に短縮される。 |
| **ストレージの利用** | RAMへの一時的なスワップのみ。ディスクへの永続化はアプリケーション側の実装に依存していた。 | **推論エンジンレベル**でSSDやネットワークストレージへのオフロードを効率的に実現。 |

NVIDIA Dynamoは、vLLMなどの人気推論エンジンともシームレスに統合可能であり、計算コストと応答速度の両面で、AI推論の効率を根本から変革する技術として注目されています。
