# KV-cache

| 要素 | 役割（モデルの処理） | 役割（料理の例え） |
| :--- | :--- | :--- |
| **パラメータ（知識）** | 一般的な知識、言語の法則。どの単語が次の単語として適切かを判断する基準。 | 料理人の内在する知識。「塩と砂糖を間違えない」「魚は焼く」といった一般的な料理の法則。 |
| **$\text{KV}$ キャッシュ（文脈）** | 現在の会話や入力プロンプトという一時的な文脈。次に生成する単語が現在の文脈に合致しているかを確認する索引。 | 現在のレシピや直前の動作という一時的な作業文脈。「今回は塩ではなく砂糖を使う」といった、この料理固有の文脈。 |

エンコーダー・デコーダー型LLMが推論を行う際、モデルの知識と文脈は明確に分離された構造を持ちます。モデルが持つ一般的な知識、言語の法則、世界の事実などは、学習済みパラメータ（重み）に永続的に保持されており、これが知識の源泉となります。一方、$\text{KV}$ は一時的な文脈情報として、推論中に生成されます。この $\text{KV}$ の生成には二つのステップがあります。まず、入力文が与えられると、エンコーダーがこのパラメータを用いて、入力文の深い意味を符号化したエンコーダー $\mathbf{KV}$ を一度だけ生成します。このエンコーダー $\mathbf{KV}$ は、元の入力文が変わらない限り固定されており、再計算の必要はありませんが、デコーダーが生成の全ステップで参照できるようにメモリ上にキャッシュされます。次に、応答の生成が始まると、デコーダーはパラメータを用いつつ、自身の自己注意機構を通じて出力履歴の $\mathbf{KV}$ を逐次生成します。このデコーダー $\mathbf{KV}$ は、新しいトークンが生成されるたびに内容が変化（成長）するため、計算の重複を避ける目的でKVキャッシュに保存・更新されます。もしこの $\text{KV}$ キャッシュがなかった場合、モデルは新しいトークンを予測するたびに、既に出力した文章全体（過去の全てのトークン）をデコーダーに再入力し、全ての $\mathbf{K}$ と $\mathbf{V}$ を最初から最後まで、すべて再計算しなければならず、計算コストが二乗で増大し、実用的な応答速度が得られなくなります。最終的に、デコーダーはこの二つの $\text{KV}$（不変のエンコーダー $\text{KV}$ と可変のデコーダー $\text{KV}$）を文脈情報として参照しつつ、自身のパラメータに保持された知識を活用することで、正確な応答を生成します。
